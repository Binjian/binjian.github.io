+++
title = "Ai Training"
author = ["Binjian Xin"]
date = 2023-01-11T00:00:00+08:00
draft = false
+++

<div class="ox-hugo-toc toc">

<div class="heading">Table of Contents</div>


</div>
<!--endtoc-->
## AI Training

### Overview {#overview}

### Definition of Intelligence {#definition-of-intelligence}

#### Biology: Behavior, (inwards), descriptive, declarative, inductive, knowing-that {#biology-behavior-inwards-descriptive-declarative-inductive-knowing-that}


#### Artificial Intelligence: Mathematics, Physics, (outwards), engineering, procedural, true understanding of intelligence, knowing-how {#artificial-intelligence-mathematics-physics-outwards-engineering-procedural-true-understanding-of-intelligence-knowing-how}

- Machine in the Ghost (Gilbert Ryle, 1949): The fallacy of dualism, category error (the distinction between mind and body)
    - Separating conscious processes (intelligence) from physical processes.
- "Can Digital Computers Think?" 1951, Turing, expected in 50 years.
    > ‘To take an extreme case, we are not interested in the fact that the brain has the consistency of cold porridge. We don’t want to say ‘‘This machine’s quite hard, so it isn’t a brain, and so it can’t think.’’ ’


#### Engineering Progress in AI as a Response to Turing's Question after 50 Years {#engineering-progress-in-ai-as-a-response-to-turings-question-after-50-years}

<rV77BJAUYGU>


### Machine Learning {#machine-learning}

#### Machines Can Learn {#machines-can-learn}

- Neural Networks (feedforward, convolutional, recurrent, transformer, GPT)
- Scaling Law and Emerging Capabilities
- Gradient Back-Propagation


#### Why Can Machines Learn? {#why-can-machines-learn}

- Agency
    - A simple universal assumption:
        - Dynamic systems, boundaries, Computing world, Dynamics + Agent (System with Boundaries)
        - Complexity assumption and irreducible computability


#### How Do Machines Learn? {#how-do-machines-learn}

- Supervised learning
- Semi-supervised learning
- Unsupervised learning
- Objective function


### Optimization - Neural Networks (Deep Learning) {#optimization---neural-networks-deep-learning}


#### Network Architecture {#network-architecture}

- Fully connected layer (feedforward network)
- Convolutional layer (convolutional network)
- Attention layer (transformer network)
- Sparse topology layer (graph neural network)


#### Optimization {#optimization}

- Constructing loss (objective function, maximum likelihood estimation)
- Gradient Back-Propagation


### Goals {#goals}

#### Supervised Learning: Maximum Likelihood Estimation {#supervised-learning-maximum-likelihood-estimation}


#### Generative Model (Dynamic stochastic process, transformation of probability distribution) {#generative-model-dynamic-stochastic-process-transformation-of-probability-distribution}

- Variational Autoencoder (VAE)
- Generative Adversarial Network (GAN)
- Diffusion Model (Dynamic stochastic process)
- Active Inference (Free energy principle, dynamic equilibrium)
    - Maximizing the objective: Confidence accuracy + entropy based on the internal model (Minimizing surprise, the expectation of impossibility)
    - Maximizing the independent variable: Parameters of the internal model


#### Kolmogorov Complexity {#kolmogorov-complexity}

- Knowledge compression as a goal
    - Markus Hutter (Shane Legg)
    - Ilya Sutskever


### Machine Learning Example {#machine-learning-example}


#### House Price Prediction as Linear Regression {#house-price-prediction-as-linear-regression}

- [Linear Regression](https://www.geeksforgeeks.org/ml-linear-regression/)
- [Linear Regression: Jupyter Notebook](https://github.com/AshishJangra27/Machine-Learning-with-Python-GFG/blob/main/Regression/1%20Linear%20Regression/Linear%20Regression%20from%20Scratch.ipynb)


### Technological Progress {#technological-progress}

{{< figure src="/ox-hugo/technology.png" >}}

> The emergence of new technologies leads to social progress, and AI is hailed as the electricity of the new era.
> Electricity has its downsides:
> 
> - Risk of electric shock.
> - Expensive infrastructure.
> - Eliminates old industries and creates new industries and professions.
> 
> Big Data:
> [Jordan Tigani (ex Google Engineering lead of BigQuery) Big Data is Dead](https://motherduck.com/blog/big-data-is-dead/)
> 2011, 2017~2019, big data did not become a bottleneck.
> 
> - Does not reach the big data level GB.
> - Storage and computing are separating.
> - No new business, data is growing linearly.
> - People only care about recent data.
> - Companies with truly big data almost never query all of it, 2017.
> - The computing power of a single machine has greatly increased.


### Updating Scientific Concepts {#updating-scientific-concepts}

{{< figure src="/ox-hugo/science.png" >}}

> Three areas are undergoing huge, persistent, and profound changes:
> 
> - The deeper the understanding of the principles, the greater the impact of the application --> revolutionary applications.
>   - Physics case: Nuclear fusion, the origin of the universe, the formation of stars, \(E=MC^2\), inexhaustible safe energy, 50 years later --> 5 years later.
>   - Biology case: Biochemical origins of eukaryotic organisms: photosynthesis, cellular respiration, mitochondria, extraterrestrial life research.
> - Discovering problems is the research direction to make progress.
> - Deep understanding will change perceptions!
> 
> [Bill Gates: The Age of AI Has Begun](https://www.gatesnotes.com/The-Age-of-AI-Has-Begun)
> 
> - The second revolutionary technology demonstration after the GUI, mid-2022 --> September. 

