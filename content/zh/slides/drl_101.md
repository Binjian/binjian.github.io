+++
title = "博弈,最优控制和机器学习"
author = ["忻斌健"]
date = 2025-01-02T00:00:00+08:00
draft = false
+++

<div class="ox-hugo-toc toc">

<div class="heading">&#30446;&#24405;</div>

- [目标](#目标)
- [一个简单的决策问题](#一个简单的决策问题)
- [最优控制，强化学习与机器人](#最优控制-强化学习与机器人)
- [总结](#总结)

</div>
<!--endtoc-->

<div class="NOTES">

-   Rip Van Winkel 华盛顿·欧文 1819，黄粱一梦，错过美国独立革命，纽约的民间故事

</div>


## 目标 {#目标}

-   最优控制和机器学习的共同基础
-   决策问题程序化
-   通过学习的方式获得最优决策

<div class="NOTES">

-   理解强化学习，不需要理解深度学习
-   可以从强化学习的角度理解深度学习的本质
-   强化学习的背景，动机，强化学习本身只占较小的一部分
-   暗物质
    -   指基本事实经常会被忽略，或下意识假定，经常是错误的假定，而对理解问题是非常必要的前提。
-   机器人抓取任务里为何会使用随机决策?违反直觉!
    -   逻辑与直觉结合处理复杂现象
-   可以用基本的问题引出
    -   为什么是树结构？
    -   什么是机器学习？

</div>


## 一个简单的决策问题 {#一个简单的决策问题}


### 1.1 对决 {#1-dot-1-对决}

> 游戏(博弈)
>
> -   游戏的要素:
>     -   玩家
>     -   收益(代价,奖励)
>     -   策略
> -   游戏的趣味：复杂，难度，平衡

<div class="NOTES">

👉 王者荣耀

</div>


#### 对决 {#对决}

<div class="r-stack">
        <img class="fragment fade-out" data-fragment-index="0" src="img/drl101/dominated_fight.png" />
        <img class="fragment current-visible" data-fragment-index="0" src="img/drl101/dominated_fight0.png" />
        <img class="fragment current-visible" data-fragment-index="1" src="img/drl101/dominated_fight1.png" />
        <img class="fragment" data-fragment-index="2" src="img/drl101/dominated_fight2.png" />
</div>
<span class="fragment"; style="color:darkgreen; font-weight:bold"; data-fragment-index="2">优势策略</span>


### 1.2 势均力敌 {#1-dot-2-势均力敌}

<div class="r-stack">
        <img class="fragment fade-out data-fragment-index="0" src="img/drl101/ne.png" />
        <img class="fragment current-visible" data-fragment-index="0" src="img/drl101/ne1.png" />
        <img class="fragment current-visible" data-fragment-index="1" src="img/drl101/ne2.png" />
        <img class="fragment" data-fragment-index="2" src="img/drl101/ne3.png" />
        <img class="fragment" data-fragment-index="4" src="img/drl101/mixed.png" style="height:400px" />
</div>
<span class="fragment"; style="color:darkgreen; font-weight:bold"; data-fragment-index="2">策略均衡状态</span>
<div class="fragment"; style="color:darkred; font-weight:bold"; data-fragment-index="3">?</div>

<div class="NOTES">

-   没有优势策略
    -   当有些问题没有答案的时候，从另一个角度或层次会发现更有趣的现象或更重要问题
-   行与列(两个对手)的策略重合，是更重要的问题
    -   一方选择攻击，另一方退出的状态下，没有任何一方愿意偏离当前的状态，平衡状态。（纳什均衡）

</div>

<div class="NOTES">

-   前提条件是同时决策,不知道对方的策略!
-   策略均衡限于稳定的平衡状态
-   策略均衡是对双方最合理的最优状态：任何一方偏离均衡状态，而另一方保持理性决策，都会导致偏离方收益受损，所以没有任何一方愿意偏离均衡状态下的最优决策
-   A 非理性决策 vs B 理性决策
-   理性决策优于非理性决策
-   有几个均衡状态？

</div>


### 1.3 混合策略 {#1-dot-3-混合策略}

<a id="figure--混合策略"></a>

{{< figure src="/ox-hugo/mixed1.png" title="混合策略" width="300px" >}}

-   对手攻击收益：\\(\color{red}{PO^{f}=(-1)\times p^{A} + (2)\times (1-p^{A})}\\)
-   对手撤退收益：\\(\color{blue}{PO^{q}=(0)\times p^{A} + (0)\times (1-p^{A})}\\)
-   \\(p^{A}=0.5\\) ?
    -   我方收益 -0.5:1
    -   对手收益：0.5:0
-   \\(p^A\\) 何时最优?

    👉 让对方失去选择, 对\\(\forall\hspace{0.5em}p^{B}\\)

    <div class="NOTES">

    -   我方的收益取决于对手的决策！
    -   对手的任何策略,收益都一样

    </div>


#### 混合策略 {#混合策略}

{{< figure src="/ox-hugo/mixed1.png" title="混合策略" width="300px" >}}

-   我方策略:\\(\color{red}{PO^{f}}=\color{blue}{PO^{q}}\\) 👉 \\(p^{A}=\frac{2}{1+2}=\frac{2}{3}\\)
    -   收益？
    -   均衡策略: 我方收益 \\(-\frac{2}{3}\times p^{B} + \frac{4}{3}\times (1-p^{B})\\)
    -   \\(p^{A}=1\\)?
-   理性决策优于非理性决策

    <div class="NOTES">

    -   A 非理性决策 (p=1,0.5) vs B 理性决策 (p=2/3)

    </div>
-   混合策略的均衡是对双方最合理的最优状态

    <div class="NOTES">

    -   混合策略的均衡：任何一方偏离均衡状态，而另一方保持理性决策，都会导致偏离方收益受损，所以没有任何一方愿意偏离均衡状态下的最优决策

    </div>
-   随机策略优于确定策略

    <div class="NOTES">

    -随机是应对复杂现象的高效模型

    -   如何从随机策略中选择一个最优的策略? 对信号的概率分布进行运算,找出符合目标的最优策略.

    </div>


#### 混合策略 {#混合策略}

{{< figure src="./img/drl101/mixed.png" title="混合策略 width 300px" >}}

-   多轮持续对决？


### 1.4 在时间的长河里 {#1-dot-4-在时间的长河里}


#### 决策树 {#决策树}

<div class="r-stack">
 <img class="fragment fade-out data-fragment-index="0" src="img/drl101/mixed1.png" />
 <img class="fragment current-visible" data-fragment-index="0" src="img/drl101/flat_tree.png" />
 <img class="fragment" data-fragment-index="1" src="img/drl101/flat_tree2.png" />
</div>

<div class="r-stack">
 <img class="fragment fade-out data-fragment-index="2" src="img/drl101/tree.png" />
 <img class="fragment" data-fragment-index="2" src="img/drl101/flat_tree3.png" />
</div>

<div class="NOTES">

-   决策树对决策理论（强化学习）, 几乎是唯一的模型
-   对理解时间序列至关重要

</div>


### 1.5 持续对决 {#1-dot-5-持续对决}

<div class="r-stack">
        <img class="fragment fade-out data-fragment-index="0" src="img/drl101/tree21.png" />
        <img class="fragment current-visible" data-fragment-index="0" src="img/drl101/tree3.png" />
        <img class="fragment" data-fragment-index="1" src="img/drl101/tree4.png" />
</div>

-   逐级倒推: 从最后一轮开始分析
-   攻击发生概率 \\(\mathcal{P}=\frac{v}{v+c}: \frac{2}{3}\searrow 0, \textrm{if}\quad v: 2\searrow 0\\)
-   价值函数：当前决策和状态的长期价值

<div class="NOTES">

-   时间序列的复杂度指数数增加！
-   生物演化也是树状
-   因果序列的分歧演化

</div>

<div class="NOTES">

-   最优决策需要考虑短期作用的长期后果
    -   存在一个基本规律(类似物理学的基本定律)，可以很容易地评估短期作用的长期后果
        -   自然界，经验积累
        -   最优控制，强化学习
-   如何评估这个后果？
    -   经验积累相似的方式，积累“价值函数”

</div>


## 最优控制，强化学习与机器人 {#最优控制-强化学习与机器人}


### 2.1 回顾 {#2-dot-1-回顾}


#### 十年前 {#十年前}

<div class="NOTES">

-   PR2
    -   2010年, willow garage (ROS吴恩达)
    -   执行器，传感器(深度相机,激光雷达)，本体,关节
    -   成本下降
    -   本体更仿生(更复杂)
-   Asimo
    -   步态控制和现在机器人的区别
    -   现在: <https://www.youtube.com/watch?v=6CjxMPg0pvg>

</div>


#### 最优控制 {#最优控制}

<div class="NOTES">

2019 talk

-   控制理论早期：PID 反馈控制， 线性控制,前提是线性系统，简洁优美
    -   无差别地运用到其他控制对象，导致复杂专家系统，复杂工程和系统,
    -   反馈控制理论:零极点补偿➡取消系统原有的动态,没有利用原有的系统动态
-   1990s,12:45, 最优控制（近似动态规划）与强化学习
    -   主要区别在于强化学习强调与环境互动，基于学习
    -   最优控制（近似动态规划, 自适应控制，鲁棒控制）强调系统辨识,模型近似
    -   浅层神经网络,没有深度学习
    -   Dimitri P. Bertsekas
-   14m18s~15m57s;
-   将会简单评论AlphaGO算法

</div>


#### 强化学习 {#强化学习}

{{< figure src="/ox-hugo/jim_fan.png" title="Year_Of_RL width 400px" >}}

{{< figure src="/ox-hugo/deepseek_r1_arxiv.png" title="R1 width 400px" >}}


### 2.2 模型 {#2-dot-2-模型}

<div class="ox-hugo-table noboldheader">

| 博弈                                                                                    | 玩家                                                                                          | 收益<br>(代价)                                                                        | 策略                                                                                  | 状态                                                                                  | 策略评估                                                                                |
|---------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|
|  <span class="fragment" data-fragment-index="1"><font color=darkblue>强化学习</font></span> |  <span class="fragment" data-fragment-index="1"><font color=darkblue>智能体/<br>系统</font></span> |  <span class="fragment" data-fragment-index="1"><font color=darkblue>奖励</font></span> |  <span class="fragment" data-fragment-index="1"><font color=darkblue>作用</font></span> |  <span class="fragment" data-fragment-index="1"><font color=darkblue>状态</font></span> |  <span class="fragment" data-fragment-index="1"><font color=darkblue>价值函数</font></span> |
|  <span class="fragment" data-fragment-index="2"><font color=red>最优控制</font></span>  |  <span class="fragment" data-fragment-index="2"><font color=red>控制器/<br>对象</font></span> |  <span class="fragment" data-fragment-index="2"><font color=red>误差</font></span>    |  <span class="fragment" data-fragment-index="2"><font color=red>控制量</font></span>  |  <span class="fragment" data-fragment-index="2"><font color=red>状态</font></span>    |  <span class="fragment" data-fragment-index="2"><font color=red>目标函数</font></span>  |

</div>

<div class="NOTES">

-   玩家：人类, vs 人类,计算机,自然/物理规律;
-   计算机 vs. 自然/物理规律
-   增加观测量和价值估计
    -   强化学习的价值函数是通过学习得到，是系统状态和作用的函数，考虑系统方程和系统动态
    -   最优控制的目标函数是专家规则，没有考虑系统动态!

</div>

<a id="figure--强化学习模型"></a>

{{< figure src="/ox-hugo/rl_model.png" title="强化学习模型" width="750pix" >}}


### 2.3 强化学习的方法 {#2-dot-3-强化学习的方法}

> 分步骤解决复杂问题

-   “如给定现在，未来与过去无关”

    👉 马尔可夫决策过程
-   复杂问题可分解为子问题

    👉 动态规划
-   从碎片化的经验中估计状态和行动价值

    👉 贝尔曼方程

<div class="NOTES">

-   理解概念比记住概念的名称更重要
-   动态规划是主流的经典概念,也是最优控制的基础
-   从a走到b的最短路径,可分两个阶段a到c,c到b:假设c到b最短,那么只要解决a到c最短这一个子问题!

</div>


#### 理性决策 {#理性决策}

> -   算法是理性决策
> -   理性决策针对非理性决策是优势策略

<div class="NOTES">

-   AlphaGo 很难战胜，人很难战胜机器：完美记忆，纯粹理性，高效执行，可复制
-   没有目的
-   Jeff Hinton的警告
-   强化学习本来是人工智能领域里一个比较冷僻的方向,和最优控制最大的差异在于学习的概念.
    -   为何自2016年以来越来成为人工智能，机器人的主流？--&gt; 深度学习。
    -   两者如何结合？采样！从碎片化经验中学习。

</div>


### 2.4 从碎片化经验中学习 {#2-dot-4-从碎片化经验中学习}


#### 随机采样 {#随机采样}

<div class="r-stack">
        <img class="fragment fade-out data-fragment-index="0" src="img/drl101/tree_sample0.png" />
        <img class="fragment current-visible" data-fragment-index="0" src="img/drl101/tree_sample.png" />
        <img class="fragment current-visible" data-fragment-index="1" src="img/drl101/tree_sample1.png" />
        <img class="fragment current-visible" data-fragment-index="2" src="img/drl101/tree_sample2.png" />
        <img class="fragment" data-fragment-index="3" src="img/drl101/tree_sample3.png" />
</div>


#### 随机采样的好处 {#随机采样的好处}

<a id="figure--随机决策采样"></a>

{{< figure src="/ox-hugo/tree_sample1.png" title="随机决策采样" width="500pix" >}}

-   真实的数据
    -   建模的复杂度过高
-   复杂函数/分布：
    -   非线性
    -   时变过程与非平稳过程
-   自然规律
-   处理复杂问题的高效方式
-   可以从碎片化的经验中学习

<div class="NOTES">

-   掷色子通常是复杂随机环境最高效的学习方式
-   数学上的确定性问题用概率方法去求解往往有简洁高效的的方式，（组合数学）

</div>


#### 最优控制 vs. 强化学习 {#最优控制-vs-dot-强化学习}

<div class="r-stack">
   <img class="fragment" data-fragment-index="0" src="img/drl101/hiking.jpg" height="400px"/>
</div>
<div class="r-stack">
   <div class="centered"><span class="fragment" data-fragment-index="0">最优控制</span></div>
</div>

<div class="r-stack">
   <img class="fragment current-visible" data-fragment-index="1" src="img/drl101/surfing.jpg" height="400px"/>
   <img class="fragment" data-fragment-index="2" src="img/drl101/skateboarding.jpg" height="400px"/>
</div>
<div class="r-stack">
   <div class="centered"><span class="fragment current-visible" data-fragment-index="1">强化学习</span></div>
   <div class="centered"><span class="fragment" data-fragment-index="2">机器人强化学习</span></div>
</div>


### 2.5 模型复杂度 {#2-dot-5-模型复杂度}


#### AlphaGo的状态和决策树 {#alphago的状态和决策树}

<a id="figure--AlphaGo决策树"></a>

{{< figure src="/ox-hugo/MCTS-in-AlphaGo.png" title="AlphaGo决策树" width="800pix" >}}

-   价值: 可理解为胜率


#### AlphaGo的状态和决策树 {#alphago的状态和决策树}

<a id="figure--AlphaGo决策树"></a>

{{< figure src="/ox-hugo/alphago_mcts.png" title="AlphaGo决策树" width="800pix" >}}


#### AlphaGo的复杂度 {#alphago的复杂度}

-   所有的位置（观测量） \\(3^{{19}^2}\approx 1.74\times 10^{172}\\), \\(1.20\\%\\) 合法
-   平均~200步/局，不同棋局的平均数量 \\(~3\times 10^{511}\\)
-   理论最长步数 \\(10^{48}\\), 不同棋局的数量:\\([10^{10^{48}},10^{10^{171}}]\\)
-   可观测宇宙的原子个数 \\(10^{80}\\)

    👉  神经网络

<div class="NOTES">

-   原子个数: 爱丁顿数
-   从完整的部份经验中学习: 从部份棋局中学习,累积学习经验
-   从不完整的部份经验中学习: 在线学习,不等棋局结束,边干边学
-   已经解决，令人惊叹！
    -   人类智慧和经验的总结：攻防，布局，死活，官子，联络，形势，手筋，攻防
    -   试图通过特征方法总结人类经验，完全不敌AlphaGo
-   人工智能的苦涩教训

</div>


#### 双足机器人的状态和复杂度 {#双足机器人的状态和复杂度}

<div class="NOTES">

-   再看一个具身智能的例子：cassie，双足机器人
-   5m57s~6m27s, 7m08s~8:45s
    -   惯性/质量矩阵正定矩阵,高复杂性
    -   系统状态与动态,策略(控制器);
    -   目标(收益,控制轨迹),策略评估,(玩家)
-   复杂性
    -   动力学系统, 控制量的长效影响
    -   部份可观测性/随机性
    -   非线性
    -   足式机器人:欠驱动系统,
        -   有意为之,控制有难度,但是更自然,更节能,自然的步态是最优的控制方案:用尽量少的能量,经济的方式进行运动控制.(控制量影响状态量的方式!)

</div>

-   复杂对象的控制方式:

    -   最优控制

        <div class="NOTES">

        -   拉格朗日力学，力/力矩➡行动/作用action:能量（动能与势能)变化的时间积分;力/能量变化产生运动;
        -   稳态作用原理(运动遵循能量均衡状态，守恒): 任何系统动态有唯一的路径
        -   自从1990s以来, 两种方法
        -   处理复杂现象没有简单有效的魔术方法，必须要消耗计算资源，关键是如何运用：或者用于系统辨识，或者用于分步分片消化经验数据。
        -   最优控制,近似动态规划 Approximate DP：精确的环境和动力学模型,抓住主要矛盾,缺点是模型的特异性,针对特殊场景和功能(难以泛化),抗干扰能力差(不健壮)

        </div>

    <!--listend-->

    -   强化学习

        <div class="NOTES">

        -   随机和概率模型,通过学习的方式(自然和人处理和解决问题的方式)
        -   系统状态,策略通过学习得到
        -   强化学习为何能处理复杂问题?

        </div>
-   如何学习?

    <div class="NOTES">

    -   主要是深度学习的突破
    -   实现里从碎片化经验中学习复杂的系统动态
    -   评估复杂价值函数,复杂的策略!

    </div>


### 2.6 机器人的机器学习 {#2-dot-6-机器人的机器学习}

-   每次演示是决策树上的一条路径
-   随机采样的数据密度
-   成功或失败的经验

<div class="NOTES">

-   强化学习训练
-   覆盖特定功能的观测数据分布
-   成功或失败的路径

</div>


#### 仿真的作用 {#仿真的作用}

<a id="figure--pick"></a>

{{< figure src="https://developer-blogs.nvidia.com/wp-content/uploads/2022/07/image16.gif" caption="<span class=\"figure-number\">&#22270;1&nbsp; </span>抓取" width="400pix" >}}

<a id="figure--position"></a>

{{< figure src="https://developer-blogs.nvidia.com/wp-content/uploads/2022/07/image5-1.gif" caption="<span class=\"figure-number\">&#22270;2&nbsp; </span>定位" title="tree" width="400pix" >}}

<a id="figure--operation"></a>

{{< figure src="https://developer-blogs.nvidia.com/wp-content/uploads/2022/07/image6.gif" caption="<span class=\"figure-number\">&#22270;3&nbsp; </span>操作" title="tree" width="400pix" >}}


#### 用于训练的仿真数据 {#用于训练的仿真数据}

<div class="NOTES">

-   15:24 ~ 16:57
-   ACRONYM Nvidia FLEX
-   想象一下用模型方式来描述
-   应用工程师的技能要求：可能不需要编程

</div>


#### 机器人学习的算法 {#机器人学习的算法}

-   数据
    -   来源:在线/离线/(仿真)
    -   预训练(基础模型GPT)
    -   数据范式(训练规划/数据/多样性构造)
-   学习模型
    -   鲁棒性
    -   多样性

<div class="NOTES">

-   学习模型:代表学习,神经网络
-   数据非常重要
    -   在线/离线
    -   预训练(基础模型GPT)
        -基础模型(常识和基础推理能力)，跨域学习（自动驾驶经验有助于人形机器人的性能）
    -   数据多样化非常重要,多形态的机器人数据更有意义:可训练同一个模型,平均性能改善50%以上
-   高效学习模型,能学习复杂行为模式(多模态)

</div>


### 2.7 理解AlphaGo {#2-dot-7-理解alphago}


#### AlphaGo系统结构 {#alphago系统结构}

<a id="figure--AlphaGo神经网络"></a>

{{< figure src="/ox-hugo/alphago_nn.png" title="AlphaGo神经网络" width="800pix" >}}

-   碎片化经验学习
    -   部分经验累积 👉 神经网络
    -   不完整经验累积 👉 在线学习
-   随机和概率是应对复杂现象的有效模型
    -   价值网络: 可理解为简单的胜率查找表

<div class="NOTES">

-   可以下完一局学一局
-   可以边下边学(时序差分学习)
-   决策网络，
-   围棋复杂度极高,但是确定性游戏

</div>


#### 最优策略 {#最优策略}

<a id="figure--AlphaGo神经网络"></a>

{{< figure src="/ox-hugo/alphago_nn.png" title="AlphaGo神经网络" width="800pix" >}}

-   均衡策略
    -   混合策略的均衡是对双方最合理的最优状态
    -   理性决策优于非理性决策
-   自我训练/自我学习
    -   不断提升水平

> → 均衡状态(最优策略)

<div class="NOTES">

-   均衡策略为何是最优的策略?
    -   混合策略的均衡：任何一方偏离均衡状态，而另一方保持理性决策，都会导致偏离方收益受损，所以没有任何一方愿意偏离均衡状态下的最优决策
    -   直觉：先立于不败之地(防御)，才能战胜对手
-   A 非理性决策(人类棋手) vs B 理性决策(AlphaGo)
-   纳什均衡: 自我博弈，我的决策必须让对手的收益在任何决策下是一样的
    -   自我训练/自我学习:左右互搏
    -   自我训练为何能提升水平
        -   数学上:在合理假设下（收益大于代价\\(v>c\\),理性决策：平衡状态对应收益的一阶导数，平衡状态的二阶导数&lt;0

</div>


## 总结 {#总结}

-   最优策略
    -   最优决策必须要考虑对手的决策
-   机器学习
    -   <span class="r-stack">
        <span class="fragment fade-out"; data-fragment-index="4">随机采样是应对复杂问题的高效方法</span>
        <span class="fragment fade-in"; style="color:#FF0000; font-weight:bold"; data-fragment-index="4">随机采样是应对复杂问题的高效方法</span>
        </span>
-   神经网络
    -   随机和概率是应对复杂现象的有效模型
